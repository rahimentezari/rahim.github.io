<!DOCTYPE html>
<html>
<head>
  <title>Using Generative Adversarial Network for Gaze Estimation</title>
  <link rel="icon" href="images/favicon2.png">
</head>
<body>


<h2>Using Generative Adversarial Network for Gaze Estimation</h2>

<p>My master thesis was about Human Gaze Estimation and as Generative Adversarial Networks are becoming hot, I was wondering If I can use of GAN for Gaze Estimation. When searched, I found the following paper, which sounds interesting to me in this context: </p>

<p>"Learning from Simulated and Unsupervised Images through Adversarial Training"<p>
This paper has been published in arxiv in 22nd Dec., meaning just 1 month before. This paper arrives less than a month after the Apple said it would no longer ban employees from publishing research relating to artificial intelligence. This paper is done by vision expert Ashish Shrivastava and a team of engineers including Tomas Pfister, Oncel Tuzel, Wenda Wang, Russ Webb and Apple Director of Artificial Intelligence Research Josh Susskind  <p>

about this paper:<p>
One of the main challenges in machine learning is real labeled data for training. Using synthetic images to train deep networks can somehow solve the problem.Using just synthetic images can be problematic, because what algorithm learn with this kind of data doesn't always cope with the real situation with real data, so the trained algorithm will fail in generalization, specially on real images.<p>
To improve the performance for testing, this paper uses so-called "Simulated+Unsupervised" learning. In fact, they use Generative Adversarial Network in order to improve the generalization ability. In GANs, there are two competing neural networks, generator and discriminator, against each other to get better results. In more details, This "S+U" is nothing but combining real image data with annotated synthetic images.<p>
<a href="#" class="image featured"><center><img src="images/gan-gaze2.png" alt="" /></center></a> <p>
  
  
They called their network SimGAN. As you can see in the picture, there are Refiner(R) and Discriminator(D). The first one tries to minimize th e combination of a "local adversarial loss" and a "self-regularization term".<p>
- The discriminator network, D, classifies an image as real or refined. The adversarial loss fools this discriminator network.<p>
- The self-regularization term minimizes the image difference between the synthetic and the refined images.<p>

for more information, including implementation details, see  <a href="https://github.com/rahimentezari/simulated-unsupervised-tensorflow">here</a>, </p
</body>

</html>
