<!DOCTYPE html>
<html>
<head>
  <title>Rahim Entezari</title>
  <link rel="icon" href="images/favicon2.png">
</head>
<body>


<h2>Generating Videos with Scene Dynamics</h2>
One of the recent works that sounds so interesting to me is <a href="http://web.mit.edu/vondrick/tinyvideo/"> "Generating Videos with Scene Dynamics"</a>. I am proud of that one of the writers is my counteryman, <a href="https://www.csee.umbc.edu/~hpirsiav/">Dr. Hamed Pirsiavash</a> <p>
The main contribution of this work is using unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). Their experiments has shown that this proposed model can generate tiny videos up to a second at full frame rate. They have used Generative Adversarial Network for video with a spatio-temporal convolutional architecture<p>
Below is an example of videos for "Beach", generated by their generative video model. As you can see, the motions are fairly resonable.<p>
<img src="images/gan-gaze-1.gif" alt="HTML5 Icon" style="width:128px;height:128px;">
<a href="#" class="image featured"><img src="images/gan-gaze2.png" alt="" /></a> <p>


for more information, including implementation details, see  <a href="https://github.com/cvondrick/videogan">here </a> </p
</body>

</html>
